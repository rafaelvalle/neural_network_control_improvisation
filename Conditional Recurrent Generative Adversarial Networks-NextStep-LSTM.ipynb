{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cpu,floatX=float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import functools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sbn\n",
    "import deepdish as dd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import neural_networks as nn\n",
    "from music_utils import generateSequence\n",
    "from nnet_utils import get_next_batch_rnn\n",
    "%matplotlib inline\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate data for interval major and minor    \n",
    "int_maj = [0, 2, 2, 1, 2, 2, 2]\n",
    "int_min = [0, 2, 1, 2, 2, 1, 2]\n",
    "min_len = 2\n",
    "max_len = len(int_maj)\n",
    "input_maj, target_maj, masks_maj = generateSequence(int_maj, min_len, max_len, clip=True)\n",
    "input_min, target_min, masks_min = generateSequence(int_min, min_len, max_len, clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_batch_size = 128\n",
    "g_batch_size = 128\n",
    "epoch_size = len(input_maj)\n",
    "n_timesteps = 7\n",
    "n_features = 12\n",
    "n_conditions = 2\n",
    "n_labels = 3\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = np.concatenate((input_maj, input_min)).astype(np.float32)\n",
    "target_data = np.concatenate((target_maj, target_min)).astype(np.float32)\n",
    "masks_data = np.concatenate((masks_maj, masks_min)).astype(np.int32)\n",
    "\n",
    "cond_data = np.zeros((2*len(masks_maj), n_features, n_conditions), dtype=np.float32)\n",
    "cond_data[:len(masks_maj), :, 0] = 1                            \n",
    "cond_data[len(masks_maj):, :, 1] = 1                         \n",
    "\n",
    "lbls_data = np.zeros((2*len(masks_maj), n_labels), dtype=np.float32)\n",
    "lbls_data[:len(masks_maj), 0] = 1                            \n",
    "lbls_data[len(masks_maj):, 1] = 1                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_nonlinearity(data, temperature=1):\n",
    "    return T.clip(lasagne.nonlinearities.softmax(lasagne.nonlinearities.linear(data / temperature)), 1e-7, 1 - 1e-7)\n",
    "    \n",
    "softmax_temperature = functools.partial(output_nonlinearity, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_specs = {'batch_size': d_batch_size,\n",
    "           'epoch_size': epoch_size,\n",
    "           'input_shape': (None, None, n_features),\n",
    "           'mask_shape': (None, None),\n",
    "           'n_output_units': n_labels,\n",
    "           'n_units': 32,\n",
    "           'n_hidden': 32,\n",
    "           'grad_clip': 100.,\n",
    "           'init': lasagne.init.HeUniform(),\n",
    "           'non_linearities': (\n",
    "              lasagne.nonlinearities.tanh,  # feedforward\n",
    "              lasagne.nonlinearities.tanh,  # feedbackward\n",
    "              softmax_temperature),  # apply sotfmax with temperature\n",
    "           'learning_rate': 0.01,           \n",
    "          }\n",
    "\n",
    "g_specs = {'batch_size': g_batch_size,\n",
    "           'epoch_size': epoch_size,\n",
    "           'input_shape': (None, None, n_features),\n",
    "           'noise_shape': (None, None, n_features),\n",
    "           'cond_shape': (None, None, n_conditions),\n",
    "           'mask_shape': (None, None),\n",
    "           'n_output_units': n_features,\n",
    "           'n_units': 32,\n",
    "           'n_hidden': 32 ,\n",
    "           'grad_clip': 100.,\n",
    "           'init': lasagne.init.HeUniform(),\n",
    "           'non_linearities': (\n",
    "              lasagne.nonlinearities.tanh,  # feedforward\n",
    "              lasagne.nonlinearities.tanh,  # feedbackward\n",
    "              softmax_temperature),  # apply sotfmax with temperature\n",
    "           'learning_rate': 0.05,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare theano variables\n",
    "d_in_X = T.ftensor3('ddata')\n",
    "d_in_M = T.imatrix('dismask')\n",
    "g_in_D = T.ftensor3('gdata')\n",
    "g_in_Z = T.ftensor3('noise')\n",
    "g_in_C = T.ftensor3('condition')\n",
    "g_in_M = T.imatrix('genmask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_discriminator(params):\n",
    "    # input layers\n",
    "    l_in = lasagne.layers.InputLayer(shape=d_specs['input_shape'], name='d_in') \n",
    "    l_mask = lasagne.layers.InputLayer(shape=d_specs['mask_shape'], name='d_mask')\n",
    "\n",
    "    # recurrent layers for bidirectional network\n",
    "    l_forward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, d_specs['n_units'], grad_clipping=d_specs['grad_clip'],\n",
    "        W_in_to_hid=d_specs['init'], W_hid_to_hid=d_specs['init'],\n",
    "        nonlinearity=d_specs['non_linearities'][0], only_return_final=True, mask_input=l_mask)\n",
    "    l_backward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, d_specs['n_units'], grad_clipping=d_specs['grad_clip'],\n",
    "        W_in_to_hid=d_specs['init'], W_hid_to_hid=d_specs['init'],        \n",
    "        nonlinearity=d_specs['non_linearities'][1], only_return_final=True, mask_input=l_mask,\n",
    "        backwards=True)\n",
    "\n",
    "    # concatenate output of forward and backward layers\n",
    "    l_concat = lasagne.layers.ConcatLayer([l_forward, l_backward])\n",
    "\n",
    "    # output layer\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=d_specs['n_output_units'], nonlinearity=d_specs['non_linearities'][2])\n",
    "\n",
    "    class Discriminator:\n",
    "        def __init__(self, l_in, l_mask, l_out):\n",
    "            self.l_in = l_in\n",
    "            self.l_mask = l_mask\n",
    "            self.l_out = l_out\n",
    "            \n",
    "    return Discriminator(l_in, l_mask, l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), \n",
    "    W_hid=lasagne.init.Orthogonal(),\n",
    "    b=lasagne.init.Constant(0.),\n",
    "    nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), \n",
    "    W_hid=lasagne.init.Orthogonal(),\n",
    "    W_cell=None, b=lasagne.init.Constant(0.),\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_generator(params):\n",
    "    # input layers\n",
    "    l_in = lasagne.layers.InputLayer(shape=g_specs['input_shape'], input_var=g_in_D, name='g_in')\n",
    "    l_noise = lasagne.layers.InputLayer(shape=g_specs['noise_shape'], input_var=g_in_Z, name='g_noise')\n",
    "    l_cond = lasagne.layers.InputLayer(shape=g_specs['cond_shape'], input_var=g_in_C, name='g_cond')\n",
    "    l_mask = lasagne.layers.InputLayer(shape=g_specs['mask_shape'], input_var=g_in_M, name='g_mask')\n",
    "    \n",
    "    # recurrent layers for bidirectional network\n",
    "    l_forward_data = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_in, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True)\n",
    "    l_forward_noise = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_noise, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True)\n",
    "    l_forward_cond = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_cond, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True)\n",
    "\n",
    "    l_backward_data = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_in, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True,\n",
    "        backwards=True)\n",
    "    l_backward_noise = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_noise, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True,\n",
    "        backwards=True)\n",
    "    l_backward_cond = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_cond, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True,\n",
    "        backwards=True)\n",
    "\n",
    "    # sum linearities of data and condition on forward and and backward recurrent layers\n",
    "    l_forward_sum = lasagne.layers.ElemwiseSumLayer([l_forward_data, l_forward_noise, l_forward_cond])\n",
    "    l_backward_sum = lasagne.layers.ElemwiseSumLayer([l_backward_data, l_backward_noise, l_backward_cond])\n",
    "\n",
    "    # apply nonlinearity to forward and backward recurrent layers\n",
    "    l_forward_nonlinearity = lasagne.layers.NonlinearityLayer(l_forward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][0])\n",
    "    l_backward_nonlinearity = lasagne.layers.NonlinearityLayer(l_backward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][1])\n",
    "\n",
    "    # concatenate output of forward and backward layers\n",
    "    l_concat = lasagne.layers.ConcatLayer(\n",
    "        [l_forward_nonlinearity, l_backward_nonlinearity])\n",
    "\n",
    "    # output layer where time is collapsed into one dimension\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=g_specs['n_output_units'], nonlinearity=g_specs['non_linearities'][2])        \n",
    "\n",
    "    class Generator:\n",
    "        def __init__(self, l_in, l_noise, l_cond, l_mask, l_out):\n",
    "            self.l_in = l_in\n",
    "            self.l_noise = l_noise\n",
    "            self.l_cond = l_cond\n",
    "            self.l_mask = l_mask\n",
    "            self.l_out = l_out\n",
    "\n",
    "    return Generator(l_in, l_noise, l_cond, l_mask, l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_training(discriminator, generator, d_specs, g_specs):\n",
    "    # get variables from discrimiator and generator\n",
    "    d_params = lasagne.layers.get_all_params(discriminator.l_out, trainable=True)\n",
    "    g_params = lasagne.layers.get_all_params(generator.l_out, trainable=True)\n",
    "    \n",
    "    d_labels = T.fmatrix('d_label')\n",
    "    g_labels = T.fmatrix('g_label')    \n",
    "\n",
    "    # G(z)\n",
    "    g_z = lasagne.layers.get_output(generator.l_out,\n",
    "                                    inputs={generator.l_in: g_in_D,\n",
    "                                            generator.l_noise: g_in_Z,\n",
    "                                            generator.l_cond: g_in_C,\n",
    "                                            generator.l_mask: g_in_M})\n",
    "\n",
    "    # create proper G(z) mask\n",
    "    g_in_M_idx = g_in_M.sum(axis=1)\n",
    "    g_in_M_alt = theano.clone(g_in_M) \n",
    "    g_in_M_alt = T.set_subtensor(g_in_M_alt[T.arange(g_in_M_alt.shape[0]), g_in_M_idx], \n",
    "                                 T.ones_like(g_in_M_alt[:, 0]))\n",
    "\n",
    "    # create proper G(z)\n",
    "    g_in_D_alt = theano.clone(g_in_D)\n",
    "    g_in_D_alt = T.set_subtensor(g_in_D_alt[T.arange(g_in_D_alt.shape[0]), g_in_M_idx], \n",
    "                                 g_z)\n",
    "\n",
    "    # D(G(z))\n",
    "    d_g_z = lasagne.layers.get_output(\n",
    "        discriminator.l_out,         \n",
    "        inputs={discriminator.l_in: g_in_D_alt, \n",
    "                discriminator.l_mask: g_in_M_alt})\n",
    "\n",
    "    g_loss = lasagne.objectives.categorical_crossentropy(1 - d_g_z, g_labels)        \n",
    "    g_loss = g_loss.mean()\n",
    "\n",
    "    g_updates = lasagne.updates.adagrad(g_loss, g_params, g_specs['learning_rate'])\n",
    "    g_train_fn = theano.function(inputs=[g_in_D, g_in_Z, g_in_C, g_in_M, g_labels],\n",
    "                                 outputs=g_loss, \n",
    "                                 updates=g_updates)\n",
    "    g_sample_fn = theano.function(inputs=[g_in_D, g_in_Z, g_in_C, g_in_M],\n",
    "                                  outputs=g_z)\n",
    "\n",
    "    # D(x)\n",
    "    d_x = lasagne.layers.get_output(discriminator.l_out,\n",
    "                                    inputs={discriminator.l_in: d_in_X, \n",
    "                                            discriminator.l_mask: d_in_M})\n",
    "  \n",
    "    d_x_loss = lasagne.objectives.categorical_crossentropy(d_x, d_labels)\n",
    "    d_g_z_loss = lasagne.objectives.categorical_crossentropy(d_g_z, g_labels)\n",
    "    d_loss = d_x_loss + d_g_z_loss\n",
    "    d_loss = d_loss.mean()    \n",
    "    \n",
    "    d_updates = lasagne.updates.adagrad(d_loss, d_params, d_specs['learning_rate'])\n",
    "    d_train_fn = theano.function(\n",
    "        inputs=[d_in_X, d_in_M, g_in_D, g_in_Z, g_in_C, g_in_M, d_labels, g_labels],\n",
    "        outputs=[d_loss, d_x_loss, d_g_z_loss, d_x, d_g_z, g_in_D_alt, g_z], \n",
    "        updates=d_updates) \n",
    "\n",
    "    d_predict_fn = None\n",
    "\n",
    "#   d_predict_fn = theano.function(\n",
    "#        inputs=[d_in_X, d_in_M],\n",
    "#        outputs=lasagne.layers.get_output(discriminator[2],\n",
    "#                                          inputs={discriminator[0]: d_in_X, \n",
    "#                                                  discriminator[1]: d_in_M}))\n",
    "\n",
    "    return d_train_fn, d_predict_fn, g_train_fn, g_sample_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_generator_batch(batch_size, inputs, conds, masks):\n",
    "    excerpt = np.random.permutation(len(inputs))[:batch_size]\n",
    "    # create random noise\n",
    "    noises = np.random.normal(size=inputs[excerpt].shape).astype('float32') \n",
    "    # create labels\n",
    "    lbls = np.zeros((batch_size, n_labels), dtype=np.float32)\n",
    "    lbls[:, -1] = 1\n",
    "    \n",
    "    return inputs[excerpt], noises, conds[excerpt], masks[excerpt], lbls\n",
    "\n",
    "\n",
    "def sample_data_batch(batch_size, inputs, targets, masks, labels, conds=None):\n",
    "    excerpt = np.random.permutation(len(inputs))[:batch_size]\n",
    "    if conds is None:\n",
    "        return inputs[excerpt], targets[excerpt], masks[excerpt], labels[excerpt]\n",
    "    else:\n",
    "        return inputs[excerpt], targets[excerpt], conds[excerpt], masks[excerpt], labels[excerpt]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(d_specs)\n",
    "generator = build_generator(g_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_train_fn, d_predict_fn, g_train_fn, g_sample_fn = build_training(discriminator, generator, d_specs, g_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre training\n",
    "n_d_iterations_pre = 0\n",
    "d_losses_pre = []\n",
    "\n",
    "folderpath = 'lstm_gan_pre_{}_dlr_{}_glr_{}_bs_{}_temp_{}'.format(\n",
    "    n_d_iterations_pre, d_specs['learning_rate'], g_specs['learning_rate'], d_batch_size, temperature)\n",
    "\n",
    "if not os.path.exists(os.path.join('images', folderpath)):\n",
    "    os.makedirs(os.path.join('images', folderpath))\n",
    "\n",
    "for i in range(n_d_iterations_pre):\n",
    "    # use same data for discriminator and generator\n",
    "    d_X, _, d_L, d_C, d_M = data_iter.next()\n",
    "    g_Z = np.random.normal(size=d_X.shape).astype('float32')     \n",
    "    g_L = np.zeros((g_Z.shape[0], n_labels), dtype=np.float32)\n",
    "    g_L[:, -1] = 1\n",
    "    g_D, g_C, g_M = d_X, d_C, d_M\n",
    "    d_loss, d_x_loss, d_g_z_loss, d_x, d_g_z, g_in_D_alt = d_train_fn(d_X, d_M, g_D, g_Z, g_C, g_M, d_L, g_L)\n",
    "    d_losses_pre.append(d_loss)\n",
    "    if i == (n_d_iterations_pre -1):\n",
    "        fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "        axes[0, 0].set_title('D Label')\n",
    "        sbn.heatmap(d_L.T, ax=axes[0, 0])\n",
    "        axes[0, 1].set_title('G Label')\n",
    "        sbn.heatmap(g_L.T, ax=axes[0, 1])\n",
    "\n",
    "        axes[1, 0].set_title('D(x)')\n",
    "        sbn.heatmap(d_x.T, ax=axes[1, 0])\n",
    "        axes[1, 1].set_title('D(G(z))')        \n",
    "        sbn.heatmap(d_g_z.T, ax=axes[1, 1])\n",
    "\n",
    "        axes[2, 0].set_title('G(z) : 0')\n",
    "        axes[2, 1].set_title('G(z) : 1')\n",
    "        axes[3, 0].set_title('G(z) : 2')\n",
    "        axes[3, 1].set_title('G(z) : 3')        \n",
    "        sbn.heatmap(g_in_D_alt[0].T, ax=axes[2, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[1].T, ax=axes[2, 1]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[2].T, ax=axes[3, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[3].T, ax=axes[3, 1]).invert_yaxis()        \n",
    "\n",
    "        axes[4, 0].set_title('Loss(d)')                        \n",
    "        axes[4, 0].plot(d_losses_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 1000/20000 [05:45<1:18:52,  4.01it/s]"
     ]
    }
   ],
   "source": [
    "# store losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "n_epochs = int(2e4)\n",
    "n_d_epochs = 5\n",
    "n_g_epochs = 1\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for i in range(n_d_epochs):\n",
    "        d_X, _,  d_M, d_L = sample_data_batch(d_specs['batch_size'], input_data, target_data, masks_data, lbls_data)\n",
    "        g_D, g_Z, g_C, g_M, g_L = sample_generator_batch(g_specs['batch_size'], input_data, cond_data, masks_data)\n",
    "        d_loss, d_x_loss, d_g_z_loss, d_x, d_g_z, g_in_D_alt, g_z = d_train_fn(d_X, d_M, g_D, g_Z, g_C, g_M, d_L, g_L)\n",
    "        d_losses.append(d_loss)\n",
    "        \n",
    "    for i in range(n_g_epochs):\n",
    "        g_D, g_Z, g_C, g_M, g_L = sample_generator_batch(g_specs['batch_size'], input_data, cond_data, masks_data)\n",
    "        g_loss = g_train_fn(g_D, g_Z, g_C, g_M, g_L)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "        axes[0, 0].set_title('D Label')\n",
    "        sbn.heatmap(d_L.T, ax=axes[0, 0])\n",
    "        axes[0, 1].set_title('G Label')\n",
    "        sbn.heatmap(g_L.T, ax=axes[0, 1])\n",
    "        \n",
    "        axes[1, 0].set_title('D(x)')\n",
    "        sbn.heatmap(d_x.T, ax=axes[1, 0])\n",
    "        axes[1, 1].set_title('D(G(z))')        \n",
    "        sbn.heatmap(d_g_z.T, ax=axes[1, 1])\n",
    "        \n",
    "        axes[2, 0].set_title('G(z) : 0')\n",
    "        axes[2, 1].set_title('G(z) : 1')\n",
    "        axes[3, 0].set_title('G(z) : 2')\n",
    "        axes[3, 1].set_title('G(z) : 3')        \n",
    "        sbn.heatmap(g_in_D_alt[0].T, ax=axes[2, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[1].T, ax=axes[2, 1]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[2].T, ax=axes[3, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[3].T, ax=axes[3, 1]).invert_yaxis()        \n",
    "        \n",
    "        axes[4, 0].set_title('Loss(d)')                        \n",
    "        axes[4, 0].plot(d_losses)\n",
    "        axes[4, 1].set_title('Loss(g)')                        \n",
    "        axes[4, 1].plot(g_losses)\n",
    "        \n",
    "        fig.savefig('images/{}/iteration_{}'.format(folderpath, iteration))\n",
    "        plt.close('all') \n",
    "    \n",
    "        display.clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
